name: Cymax Parallel Sitemap Scraper (Selenium v1.2 - FIXED)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Cymax base URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "500"
      max_workers:
        description: "Parallel browsers per job"
        default: "3"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}

          [ "$TOTAL" -eq 0 ] && TOTAL=3
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    timeout-minutes: 120

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            google-chrome-stable \
            chromium-chromedriver \
            xvfb \
            lftp

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            selenium==4.15.2 \
            webdriver-manager==4.0.1 \
            pandas==2.1.4 \
            beautifulsoup4==4.12.3 \
            lxml==4.9.4 \
            requests==2.31.0

      - name: Start Xvfb display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3
          export DISPLAY=:99

      - name: Verify Chrome versions
        run: |
          google-chrome --version
          chromedriver --version
          which google-chrome
          which chromedriver

      - name: Run scraper
        env:
          DISPLAY: :99
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          python cymax_scraper/main.py

      - name: Upload chunk
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_chunk_${{ matrix.offset }}
          path: cymax_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          shopt -s nullglob
          FIRST=$(ls chunks/*/*.csv | head -n 1)
          if [ -n "$FIRST" ]; then
            head -n 1 "$FIRST" > cymax_full.csv
            for f in chunks/*/*.csv; do
              tail -n +2 "$f" | grep -v '^$' >> cymax_full.csv
            done
            echo "Merged $(wc -l < cymax_full.csv) rows"
          else
            echo "No CSV files found"
            echo "Ref Product URL,Ref Product ID,Ref Varient ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > cymax_full.csv
          fi

      - name: Build output filename
        id: meta
        run: |
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|/||g')
          DATE=$(date +%F)
          echo "name=cymax_${SITE}_${DATE}.csv" >> $GITHUB_OUTPUT

      - name: Upload final CSV
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_complete_dataset
          path: cymax_full.csv

      - name: Upload to FTP
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          sudo apt-get update && sudo apt-get install -y lftp
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put cymax_full.csv -o $FILE
          bye
          EOF
