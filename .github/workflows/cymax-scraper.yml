name: Cymax Parallel Sitemap Scraper (Selenium v1.2)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Cymax base URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "500"
      max_workers:
        description: "Parallel browsers per job"
        default: "3"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}

          [ "$TOTAL" -eq 0 ] && TOTAL=3
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: plan
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4
      
      # CRITICAL: Chrome FIRST before Python deps
      - name: Setup Chrome and ChromeDriver
        id: chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
          install-chromedriver: true
          install-dependencies: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.1.4 selenium==4.15.2 beautifulsoup4==4.12.3 lxml==4.9.4 requests==2.31.0 webdriver-manager==4.0.1 

      - name: Debug environment
        run: |
          echo "Chrome path: $(which google-chrome)"
          echo "Chromedriver: $(which chromedriver)"
          CHROMEDRIVER_PATH=$(find /opt/hostedtoolcache -name chromedriver -type f 2>/dev/null | head -1)
          echo "Full ChromeDriver: $CHROMEDRIVER_PATH"
          echo "CHROMEDRIVER_PATH=$CHROMEDRIVER_PATH" >> $GITHUB_ENV

      - name: Run scraper
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          SELENIUM_MANAGER_DISABLED: "true"
          CHROME_BIN: /usr/bin/google-chrome
          CHROMEDRIVER_PATH: ${{ env.CHROMEDRIVER_PATH }}
          SELENIUM_MANAGER_DRIVER: "false"
        run: |
          python cymax_scraper/main.py

      - name: Upload chunk
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_chunk_${{ matrix.offset }}
          path: cymax_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          shopt -s nullglob
          FIRST=$(ls chunks/*/*.csv | head -n 1)
          if [ -n "$FIRST" ]; then
            head -n 1 "$FIRST" > cymax_full.csv
            for f in chunks/*/*.csv; do
              tail -n +2 "$f" | grep -v '^$' >> cymax_full.csv
            done
            echo "Merged $(wc -l < cymax_full.csv) rows"
          else
            echo "No CSV files found"
            echo "url,title,price,sku" > cymax_full.csv
          fi

      - name: Build output filename
        id: meta
        run: |
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|/||g')
          DATE=$(date +%F)
          echo "name=cymax_${SITE}_${DATE}.csv" >> $GITHUB_OUTPUT

      - name: Upload final CSV
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_complete_dataset
          path: cymax_full.csv

      - name: Upload to FTP
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          sudo apt-get update && sudo apt-get install -y lftp
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put cymax_full.csv -o $FILE
          bye
          EOF
