name: Cymax Scraper (Simplified Chrome Setup)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Cymax base URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "200"
      max_workers:
        description: "Parallel browsers per job"
        default: "2"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}

          [ "$TOTAL" -eq 0 ] && TOTAL=3
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    timeout-minutes: 120

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome & ChromeDriver (Complete Package)
        run: |
          echo "=== Installing Chrome and ChromeDriver ==="
          
          # 1. Install Chrome from Google repository (most reliable)
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          
          # Add Google Chrome repository
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          echo "Chrome installed:"
          google-chrome --version
          
          # 2. Get Chrome version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')
          echo "Chrome version: $CHROME_VERSION"
          
          # 3. Install matching ChromeDriver using webdriver-manager via Python
          echo "Installing ChromeDriver..."
          python -c "
          from webdriver_manager.chrome import ChromeDriverManager
          from webdriver_manager.core.os_manager import ChromeType
          import os
          
          print('Downloading ChromeDriver via webdriver-manager...')
          
          # Force clean installation
          os.system('rm -rf ~/.wdm 2>/dev/null || true')
          
          # Get ChromeDriver
          driver_path = ChromeDriverManager(chrome_type=ChromeType.GOOGLE).install()
          print(f'ChromeDriver installed at: {driver_path}')
          
          # Make a system-wide copy
          import shutil
          shutil.copy(driver_path, '/usr/local/bin/chromedriver')
          os.chmod('/usr/local/bin/chromedriver', 0o755)
          print('Copied to /usr/local/bin/chromedriver')
          "
          
          echo "ChromeDriver installed:"
          /usr/local/bin/chromedriver --version

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libnss3 libatk1.0-0 libatk-bridge2.0-0 \
            libcups2 libdrm2 libxkbcommon0 \
            libxcomposite1 libxdamage1 libxrandr2 \
            libgbm1 libasound2 libpangocairo-1.0-0 \
            libpango-1.0-0 libcairo2 libgdk-pixbuf2.0-0 \
            xvfb

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            selenium==4.15.2 \
            webdriver-manager==4.0.1 \
            pandas==2.1.4 \
            beautifulsoup4==4.12.3 \
            lxml==4.9.4

      - name: Start Virtual Display (for headless)
        run: |
          # Start Xvfb for headless display
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3
          export DISPLAY=:99
          echo "DISPLAY=:99" >> $GITHUB_ENV

      - name: Verify Setup
        run: |
          echo "=== Final Verification ==="
          echo "Chrome: $(which google-chrome)"
          google-chrome --version
          echo ""
          echo "ChromeDriver: $(which chromedriver)"
          chromedriver --version
          echo ""
          
          # Quick Python test
          echo "=== Quick Selenium Test ==="
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.chrome.service import Service
          import os
          
          print('Testing Selenium setup...')
          
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          options.add_argument('--window-size=1920,1080')
          
          # Use system ChromeDriver
          service = Service('/usr/local/bin/chromedriver')
          
          try:
              driver = webdriver.Chrome(service=service, options=options)
              driver.get('https://www.google.com')
              print(f'✓ Success! Page title: {driver.title}')
              print(f'✓ Page size: {len(driver.page_source)} characters')
              
              # Test scrolling
              driver.execute_script('window.scrollTo(0, 100)')
              print('✓ JavaScript execution works')
              
              driver.quit()
              print('✓ Browser cleanup successful')
              print('')
              print('✓ ALL TESTS PASSED - Ready to scrape!')
              
          except Exception as e:
              print(f'✗ Error: {e}')
              import traceback
              traceback.print_exc()
              print('')
              print('⚠ Setup verification failed')
          "

      - name: Run scraper
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          echo "=== Starting Scraper ==="
          echo "Configuration:"
          echo "  Base URL: $CURR_URL"
          echo "  Sitemap offset: $SITEMAP_OFFSET"
          echo "  Max sitemaps: $MAX_SITEMAPS"
          echo "  Max URLs per sitemap: $MAX_URLS_PER_SITEMAP"
          echo "  Parallel workers: $MAX_WORKERS"
          echo ""
          
          # Run with virtual display
          export DISPLAY=:99
          python main.py

      - name: Upload chunk
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_chunk_${{ matrix.offset }}
          path: cymax_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          echo "=== Merging CSV Files ==="
          
          # Create header
          echo "Ref Product URL,Ref Product ID,Ref Varient ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > cymax_full.csv
          
          # Count and merge
          COUNT=0
          for file in $(find chunks -name "cymax_chunk_*.csv" -type f | sort); do
            if [ -f "$file" ] && [ -s "$file" ]; then
              ROWS=$(( $(wc -l < "$file") - 1 ))
              if [ $ROWS -gt 0 ]; then
                tail -n +2 "$file" >> cymax_full.csv
                COUNT=$((COUNT + ROWS))
                echo "  Added $ROWS rows from $(basename $file)"
              fi
            fi
          done
          
          echo ""
          echo "✓ Merged $COUNT total rows into cymax_full.csv"

      - name: Upload final CSV
        uses: actions/upload-artifact@v4
        with:
          name: cymax_complete_dataset
          path: cymax_full.csv

      - name: Show Results
        run: |
          echo "=== Scraping Results ==="
          if [ -f cymax_full.csv ]; then
            ROWS=$(( $(wc -l < cymax_full.csv) - 1 ))
            echo "Total products scraped: $ROWS"
            if [ $ROWS -gt 0 ]; then
              echo ""
              echo "Sample of first product:"
              head -2 cymax_full.csv | tail -1 | cut -d',' -f1-3
            fi
          else
            echo "No output CSV generated"
          fi
