name: Cymax Parallel Sitemap Scraper (Selenium v1.3)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Cymax base URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "500"
      max_workers:
        description: "Parallel browsers per job"
        default: "3"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}

          [ "$TOTAL" -eq 0 ] && TOTAL=3
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    timeout-minutes: 120
    env:
      DISPLAY: :99
      LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/libproxy

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome & Dependencies
        run: |
          # Install Chrome dependencies
          sudo apt-get update
          sudo apt-get install -y \
            wget gnupg \
            ca-certificates \
            libnss3 \
            libatk1.0-0 \
            libatk-bridge2.0-0 \
            libcups2 \
            libdrm2 \
            libdbus-1-3 \
            libxkbcommon0 \
            libxcomposite1 \
            libxdamage1 \
            libxrandr2 \
            libgbm1 \
            libasound2 \
            libpangocairo-1.0-0 \
            libpango-1.0-0 \
            libcairo2 \
            libgdk-pixbuf2.0-0

          # Download and install Chrome
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt-get install -y ./google-chrome-stable_current_amd64.deb
          rm google-chrome-stable_current_amd64.deb

          # Install ChromeDriver
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
          wget -q "https://storage.googleapis.com/chrome-for-testing-public/$CHROME_VERSION.0.0/linux64/chromedriver-linux64.zip"
          unzip -q chromedriver-linux64.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          chmod +x /usr/local/bin/chromedriver
          rm -rf chromedriver-linux64.zip chromedriver-linux64

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            selenium==4.15.2 \
            webdriver-manager==4.0.1 \
            pandas==2.1.4 \
            beautifulsoup4==4.12.3 \
            lxml==4.9.4 \
            requests==2.31.0

      - name: Start Xvfb display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3
          echo "DISPLAY=:99" >> $GITHUB_ENV

      - name: Verify Chrome (Clean)
        run: |
          export DISPLAY=:99
          export LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
          
          echo "=== Chrome Version ==="
          google-chrome --version
          
          echo "=== ChromeDriver Version ==="
          chromedriver --version
          
          echo "=== Paths ==="
          which google-chrome
          which chromedriver
          
          echo "=== Headless Test ==="
          timeout 10 google-chrome --headless=new --no-sandbox --disable-gpu --dump-dom https://www.google.com >/dev/null 2>&1 || echo "✓ Headless Chrome test passed"
          
          echo "Chrome setup clean ✓"

      - name: Run scraper
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          python cymax_scraper/main.py

      - name: Upload chunk
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_chunk_${{ matrix.offset }}
          path: cymax_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          shopt -s nullglob
          FIRST=$(ls chunks/*/*.csv | head -n 1)
          if [ -n "$FIRST" ]; then
            head -n 1 "$FIRST" > cymax_full.csv
            for f in chunks/*/*.csv; do
              tail -n +2 "$f" | grep -v '^$' >> cymax_full.csv
            done
            echo "Merged $(wc -l < cymax_full.csv) rows"
          else
            echo "No CSV files found"
            echo "Ref Product URL,Ref Product ID,Ref Varient ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > cymax_full.csv
          fi

      - name: Build output filename
        id: meta
        run: |
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|/||g')
          DATE=$(date +%F)
          echo "name=cymax_${SITE}_${DATE}.csv" >> $GITHUB_OUTPUT

      - name: Upload final CSV
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_complete_dataset
          path: cymax_full.csv

      - name: Upload to FTP
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          sudo apt-get update && sudo apt-get install -y lftp
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put cymax_full.csv -o $FILE
          bye
          EOF
