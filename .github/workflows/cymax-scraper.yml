name: Cymax Scraper (Optimized Chrome Setup)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Cymax base URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "200"
      max_workers:
        description: "Parallel browsers per job"
        default: "2"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}

          [ "$TOTAL" -eq 0 ] && TOTAL=3
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    timeout-minutes: 120

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome & ChromeDriver
        run: |
          # Install dependencies
          sudo apt-get update
          sudo apt-get install -y \
            wget gnupg ca-certificates unzip curl \
            libnss3 libatk1.0-0 libatk-bridge2.0-0 \
            libcups2 libdrm2 libxkbcommon0 \
            libxcomposite1 libxdamage1 libxrandr2 \
            libgbm1 libasound2 libpangocairo-1.0-0 \
            libpango-1.0-0 libcairo2 libgdk-pixbuf2.0-0

          # Install Chrome
          echo "Installing Chrome..."
          wget -q -O /tmp/chrome.deb \
            "https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb"
          sudo apt-get install -y /tmp/chrome.deb
          rm /tmp/chrome.deb

          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')
          echo "Chrome version: $CHROME_VERSION"
          
          # Extract major version for ChromeDriver
          MAJOR_VERSION=$(echo $CHROME_VERSION | cut -d. -f1)
          echo "Major version: $MAJOR_VERSION"

          # Install matching ChromeDriver
          echo "Installing ChromeDriver..."
          
          # Method 1: Use Chrome for Testing API (most reliable)
          LATEST_VERSIONS=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions.json")
          STABLE_VERSION=$(echo $LATEST_VERSIONS | grep -o '"version":"[^"]*"' | head -1 | cut -d'"' -f4)
          
          echo "Latest stable Chrome for Testing: $STABLE_VERSION"
          
          # Download ChromeDriver
          CHROMEDRIVER_URL="https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$STABLE_VERSION/linux64/chromedriver-linux64.zip"
          echo "Downloading ChromeDriver from: $CHROMEDRIVER_URL"
          
          curl -L "$CHROMEDRIVER_URL" -o /tmp/chromedriver.zip
          unzip -q /tmp/chromedriver.zip -d /tmp/
          
          # Move chromedriver binary to /usr/local/bin
          find /tmp/chromedriver-linux64 -name "chromedriver" -type f -exec sudo mv {} /usr/local/bin/chromedriver \;
          sudo chmod +x /usr/local/bin/chromedriver
          
          # Clean up
          rm -rf /tmp/chromedriver.zip /tmp/chromedriver-linux64
          
          echo "ChromeDriver installed:"
          chromedriver --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            selenium==4.15.2 \
            webdriver-manager==4.0.1 \
            pandas==2.1.4 \
            beautifulsoup4==4.12.3 \
            lxml==4.9.4

      - name: Verify Chrome Setup
        run: |
          echo "=== System Verification ==="
          echo "Chrome: $(which google-chrome)"
          google-chrome --version
          echo ""
          echo "ChromeDriver: $(which chromedriver)"
          chromedriver --version
          echo ""
          
          echo "=== Python Selenium Test ==="
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.chrome.service import Service
          
          print('Testing Selenium setup...')
          
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          
          # Use the system ChromeDriver we installed
          service = Service('/usr/local/bin/chromedriver')
          
          try:
              driver = webdriver.Chrome(service=service, options=options)
              driver.get('https://www.google.com')
              print(f'✓ Success! Page title: {driver.title}')
              print(f'✓ Page size: {len(driver.page_source)} characters')
              driver.quit()
              print('✓ Test completed successfully')
          except Exception as e:
              print(f'✗ Error: {e}')
              import traceback
              traceback.print_exc()
          "

      - name: Run scraper
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          echo "Starting scraper with configuration:"
          echo "  Base URL: $CURR_URL"
          echo "  Sitemap offset: $SITEMAP_OFFSET"
          echo "  Max sitemaps per job: $MAX_SITEMAPS"
          echo "  Max URLs per sitemap: $MAX_URLS_PER_SITEMAP"
          echo "  Parallel workers: $MAX_WORKERS"
          echo ""
          
          python main.py

      - name: Upload chunk
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cymax_chunk_${{ matrix.offset }}
          path: cymax_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          echo "Merging CSV chunks..."
          
          # Create empty file with header
          echo "Ref Product URL,Ref Product ID,Ref Varient ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > cymax_full.csv
          
          # Find and merge all CSV files
          find chunks -name "*.csv" -type f | while read file; do
            echo "Processing: $file"
            # Skip header and append content
            tail -n +2 "$file" >> cymax_full.csv 2>/dev/null || true
          done
          
          # Count rows (excluding header)
          ROW_COUNT=$(( $(wc -l < cymax_full.csv) - 1 ))
          echo "Merged $ROW_COUNT rows into cymax_full.csv"

      - name: Upload final CSV
        uses: actions/upload-artifact@v4
        with:
          name: cymax_complete_dataset
          path: cymax_full.csv
          retention-days: 7

      - name: Upload to FTP
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
        run: |
          sudo apt-get update && sudo apt-get install -y lftp
          
          # Create filename with date
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|[/:]|-|g')
          DATE=$(date +%Y%m%d_%H%M%S)
          FILENAME="cymax_${SITE}_${DATE}.csv"
          
          echo "Uploading to FTP as: $FILENAME"
          
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ssl:verify-certificate no
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put cymax_full.csv -o $FILENAME
          ls -la
          bye
          EOF
          
          echo "FTP upload completed: $FILENAME"
