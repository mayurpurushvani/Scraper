name: Coleman and Homegallerystores Parallel Sitemap Scraper

on:
  workflow_dispatch:
    inputs:
      website_url:
        description: "Website URL to scrape"
        required: true
        default: "https://colemanfurniture.com"
        type: string
      total_sitemaps:
        description: "Total sitemaps to process (0 = all)"
        default: "0"
        type: number
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
        type: number
      urls_per_sitemap:
        description: "Max URLs per sitemap (0 = all)"
        default: "500"
        type: number
      max_workers:
        description: "Max concurrent workers per job"
        default: "3"
        type: number

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      domain: ${{ steps.domain.outputs.domain }}
    steps:
      - name: Extract domain
        id: domain
        run: |
          URL="${{ github.event.inputs.website_url }}"
          DOMAIN=$(echo "$URL" | sed -E 's|https?://||;s|/.*||' | sed 's/[^a-zA-Z0-9]/_/g')
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT
          echo "Domain: $DOMAIN"
          
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL="${{ github.event.inputs.website_url }}"
          DOMAIN="${{ steps.domain.outputs.domain }}"

          # Auto-detect if total is 0
          if [ "$TOTAL" -eq 0 ]; then
            echo "Auto-detecting sitemap count..."
            # Try to fetch sitemap index
            SITEMAP_URL="${URL}/sitemap.xml"
            if curl -s -f -o /dev/null "$SITEMAP_URL"; then
              # Count sitemaps in index
              TOTAL=$(curl -s "$SITEMAP_URL" | grep -o "<loc>[^<]*</loc>" | wc -l)
              echo "Found $TOTAL sitemaps"
            else
              TOTAL=5  # Fallback
              echo "Could not detect, using fallback: $TOTAL"
            fi
          fi
          
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))
          [ $JOBS -eq 0 ] && JOBS=1

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            LIMIT=$PER_JOB
            # Last job gets remaining
            if [ $i -eq $((JOBS - 1)) ]; then
              LIMIT=$(( TOTAL - OFFSET ))
            fi
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$LIMIT,\"url\":\"$URL\",\"domain\":\"$DOMAIN\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Total sitemaps: $TOTAL, Jobs: $JOBS"

  # Initialize tracking files
  init_tracking:
    needs: plan
    runs-on: ubuntu-latest
    steps:
      - name: Create tracking directory
        run: mkdir -p tracking
      
      - name: Initialize master tracking file
        run: |
          DOMAIN="${{ needs.plan.outputs.domain }}"
          MASTER_FILE="tracking/processed_urls_${DOMAIN}_master.json"
          
          # Create empty master file if it doesn't exist
          if [ ! -f "$MASTER_FILE" ]; then
            echo '{"urls": [], "total": 0, "last_updated": 0}' > "$MASTER_FILE"
            echo "Created new master file: $MASTER_FILE"
          else
            echo "Master file already exists: $MASTER_FILE"
            cat "$MASTER_FILE"
          fi
      
      - name: Upload tracking files
        uses: actions/upload-artifact@v4
        with:
          name: tracking-files-${{ needs.plan.outputs.domain }}
          path: tracking/
          retention-days: 1

  scrape:
    needs: [plan, init_tracking]
    runs-on: ubuntu-latest
    
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "colemanfurniture_scraper/requirements.txt" ]; then
            pip install -r colemanfurniture_scraper/requirements.txt
          else
            pip install scrapy pandas lxml requests beautifulsoup4
          fi
          
      - name: Create output directory
        run: mkdir -p output tracking
          
      - name: Download tracking files
        uses: actions/download-artifact@v4
        with:
          name: tracking-files-${{ matrix.domain }}
          path: tracking/
          
      - name: Run scraper job with tracking
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          echo "=== Job ${{ strategy.job-index }} ==="
          echo "Website: $CURR_URL"
          echo "Offset: $SITEMAP_OFFSET"
          echo "Max sitemaps: $MAX_SITEMAPS"
          echo "Max URLs per sitemap: $MAX_URLS_PER_SITEMAP"
          echo "Max workers: $MAX_WORKERS"
          echo "Domain: ${{ matrix.domain }}"
          
          # Copy master tracking file to current directory
          cp tracking/processed_urls_*_master.json ./ 2>/dev/null || echo "No master file found"
          
          # Check if script exists
          if [ ! -f "colemanfurniture_scraper/scripts/run.py" ]; then
            echo "‚ùå Script not found: colemanfurniture_scraper/scripts/run.py"
            exit 1
          fi
          
          # Run the scraper
          python colemanfurniture_scraper/scripts/run.py \
            --website-url "$CURR_URL" \
            --sitemap-offset $SITEMAP_OFFSET \
            --max-sitemaps $MAX_SITEMAPS \
            --max-urls-per-sitemap $MAX_URLS_PER_SITEMAP \
            --job-id "job${{ strategy.job-index }}" \
            --output-dir output
          
      - name: Rename output file
        run: |
          echo "Renaming output file..."
          mkdir -p output
          
          # Check if any CSV files exist
          if ls output/*.csv 1> /dev/null 2>&1; then
            CSV_FILE=$(ls output/*.csv | head -n 1)
            echo "Found CSV: $CSV_FILE"
            NEW_NAME="output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv"
            mv "$CSV_FILE" "$NEW_NAME"
            echo "Renamed to: $NEW_NAME"
          else
            echo "No CSV files found in output directory"
            # Create empty file with headers
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > "output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv"
          fi
          
      - name: Save tracking files
        run: |
          # Save any tracking files generated by this job
          cp processed_urls_*.json tracking/ 2>/dev/null || echo "No tracking files to save"
          
      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.domain }}_offset_${{ matrix.offset }}
          path: output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv
          retention-days: 1
          
      - name: Upload job tracking files
        uses: actions/upload-artifact@v4
        with:
          name: tracking-${{ matrix.domain }}-offset-${{ matrix.offset }}
          path: processed_urls_*.json
          retention-days: 1

  # Merge tracking files
  merge_tracking:
    needs: [plan, scrape]
    if: ${{ always() }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all tracking files
        uses: actions/download-artifact@v4
        with:
          path: all-tracking
          
      - name: Merge all tracking files
        run: |
          echo "=== Merging Tracking Files ==="
          
          DOMAIN="${{ needs.plan.outputs.domain }}"
          MASTER_FILE="processed_urls_${DOMAIN}_master.json"
          
          # Find all tracking files for this domain
          TRACKING_FILES=$(find all-tracking -name "processed_urls_${DOMAIN}_*.json" -type f ! -name "*_master.json")
          
          if [ -z "$TRACKING_FILES" ]; then
            echo "No tracking files found to merge"
            exit 0
          fi
          
          echo "Found $(echo "$TRACKING_FILES" | wc -l) tracking files"
          
          # Start with empty set
          ALL_URLS="[]"
          TOTAL=0
          
          # Merge all URLs
          for f in $TRACKING_FILES; do
            echo "Processing: $f"
            # Extract URLs and add to set
            while read -r url; do
              if [ -n "$url" ] && ! echo "$ALL_URLS" | grep -q "\"$url\""; then
                ALL_URLS=$(echo "$ALL_URLS" | jq --arg url "$url" '. + [$url]')
                TOTAL=$((TOTAL + 1))
              fi
            done < <(jq -r '.urls[]?' "$f" 2>/dev/null || echo "")
          done
          
          # Save merged master file
          echo "{\"urls\": $ALL_URLS, \"total\": $TOTAL, \"last_updated\": $(date +%s)}" > "$MASTER_FILE"
          
          echo "‚úÖ Master file created with $TOTAL unique URLs"
          
      - name: Upload master tracking file
        uses: actions/upload-artifact@v4
        with:
          name: tracking-master-${{ needs.plan.outputs.domain }}
          path: processed_urls_*_master.json
          retention-days: 7

  merge:
    needs: [plan, scrape, merge_tracking]
    runs-on: ubuntu-latest
    outputs:
      filename: ${{ steps.meta.outputs.name }}
      
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks
          
      - name: Download master tracking file
        uses: actions/download-artifact@v4
        with:
          name: tracking-master-${{ needs.plan.outputs.domain }}
          path: tracking/
          
      - name: Merge CSVs
        run: |
          echo "=== Merging CSV files ==="
          DOMAIN="${{ needs.plan.outputs.domain }}"
          
          # Find all CSV files for this domain
          CSV_FILES=$(find chunks -name "${DOMAIN}_offset_*.csv" -type f)
          
          if [ -z "$CSV_FILES" ]; then
            echo "No CSV files found to merge"
            # Create empty CSV with headers
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > products_full.csv
            echo "Created empty CSV"
          else
            echo "Found $(echo "$CSV_FILES" | wc -l) CSV files"
            
            # Get first file for headers
            FIRST_FILE=$(echo "$CSV_FILES" | head -n 1)
            echo "Using headers from: $FIRST_FILE"
            
            # Extract headers (first line)
            head -n 1 "$FIRST_FILE" > products_full.csv
            
            # Merge all files (skip headers)
            TOTAL_ROWS=0
            for f in $CSV_FILES; do
              echo "Merging: $f"
              if [ -s "$f" ] && [ "$(wc -l < "$f")" -gt 1 ]; then
                FILE_ROWS=$(tail -n +2 "$f" | sed '/^$/d' | wc -l)
                TOTAL_ROWS=$((TOTAL_ROWS + FILE_ROWS))
                tail -n +2 "$f" | sed '/^$/d' >> products_full.csv
              fi
            done
            
            echo "Merged $TOTAL_ROWS rows"
          fi
          
          # Show tracking stats
          if [ -f "tracking/processed_urls_*_master.json" ]; then
            MASTER_FILE=$(ls tracking/processed_urls_*_master.json | head -n 1)
            TOTAL_UNIQUE=$(jq '.total' "$MASTER_FILE")
            echo "üìä Total unique URLs processed across all jobs: $TOTAL_UNIQUE"
          fi
          
      - name: Build output filename
        id: meta
        run: |
          # Extract the original URL from the plan step or input
          ORIGINAL_URL="${{ github.event.inputs.website_url }}"
          # Extract domain without protocol and replace dots with underscores
          CLEAN_DOMAIN=$(echo "$ORIGINAL_URL" | sed -E 's|https?://||' | sed 's|/.*||' | sed 's/\./_/g')
          DATE=$(date +%F)
          FILENAME="${CLEAN_DOMAIN}_${DATE}.csv"
          echo "name=$FILENAME" >> $GITHUB_OUTPUT
          echo "Generated filename: $FILENAME"
          
      - name: Upload final CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.name }}
          path: products_full.csv
          retention-days: 7
          
      - name: Upload to FTP server
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          echo "=== Uploading to FTP ==="
          echo "FTP Host: $FTP_HOST"
          echo "File: $FILE"
          
          # Install lftp
          sudo apt-get update && sudo apt-get install -y lftp
          
          # Upload file
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ftp:ssl-allow no
          set ssl:verify-certificate no
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put products_full.csv -o $FILE
          ls -la $FILE
          bye
          EOF
          
          echo "‚úÖ File uploaded successfully: $FILE"
          
      - name: Send notification
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            PRODUCT_COUNT=$(tail -n +2 products_full.csv 2>/dev/null | wc -l || echo "0")
            echo "‚úÖ Coleman scraper completed successfully!"
            echo "Total products scraped: $PRODUCT_COUNT"
            
            # Show tracking stats
            if [ -f "tracking/processed_urls_*_master.json" ]; then
              MASTER_FILE=$(ls tracking/processed_urls_*_master.json | head -n 1)
              TOTAL_UNIQUE=$(jq '.total' "$MASTER_FILE")
              echo "Unique URLs processed: $TOTAL_UNIQUE"
            fi
          else
            echo "‚ùå Coleman scraper failed with status: ${{ job.status }}"
          fi
          
      - name: Cleanup
        if: always()
        run: |
          rm -rf chunks output tracking all-tracking
          rm -f products_full.csv processed_urls_*.json
          echo "Cleanup completed"