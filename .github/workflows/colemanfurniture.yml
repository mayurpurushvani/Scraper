name: Coleman and Homegallerystores Parallel Sitemap Scraper

on:
  workflow_dispatch:
    inputs:
      website_url:
        description: "Website URL to scrape"
        required: true
        default: "https://colemanfurniture.com"
        type: string
      total_sitemaps:
        description: "Total sitemaps to process (0 = all)"
        default: "0"
        type: number
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
        type: number
      urls_per_sitemap:
        description: "Max URLs per sitemap (0 = all)"
        default: "500"
        type: number
      max_workers:
        description: "Max concurrent workers per job"
        default: "3"
        type: number

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      domain: ${{ steps.domain.outputs.domain }}
    steps:
      - name: Extract domain
        id: domain
        run: |
          URL="${{ github.event.inputs.website_url }}"
          DOMAIN=$(echo "$URL" | sed -E 's|https?://||;s|/.*||' | sed 's/[^a-zA-Z0-9]/_/g')
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT
          echo "Domain: $DOMAIN"
          
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL="${{ github.event.inputs.website_url }}"
          DOMAIN="${{ steps.domain.outputs.domain }}"

          # Auto-detect if total is 0
          if [ "$TOTAL" -eq 0 ]; then
            echo "Auto-detecting sitemap count..."
            # Try to fetch sitemap index
            SITEMAP_URL="${URL}/sitemap.xml"
            if curl -s -f -o /dev/null "$SITEMAP_URL"; then
              # Count sitemaps in index
              TOTAL=$(curl -s "$SITEMAP_URL" | grep -o "<loc>[^<]*</loc>" | wc -l)
              echo "Found $TOTAL sitemaps"
            else
              TOTAL=5  # Fallback
              echo "Could not detect, using fallback: $TOTAL"
            fi
          fi
          
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))
          [ $JOBS -eq 0 ] && JOBS=1

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            LIMIT=$PER_JOB
            # Last job gets remaining
            if [ $i -eq $((JOBS - 1)) ]; then
              LIMIT=$(( TOTAL - OFFSET ))
            fi
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$LIMIT,\"url\":\"$URL\",\"domain\":\"$DOMAIN\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Total sitemaps: $TOTAL, Jobs: $JOBS"

  scrape:
    needs: plan
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "colemanfurniture_scraper/requirements.txt" ]; then
            pip install -r colemanfurniture_scraper/requirements.txt
          else
            pip install scrapy pandas lxml requests beautifulsoup4
          fi
          
      - name: Create output directory
        run: mkdir -p output
          
      - name: Run scraper job
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
        run: |
          echo "=== Job ${{ strategy.job-index }} ==="
          echo "Website: $CURR_URL"
          echo "Offset: $SITEMAP_OFFSET"
          echo "Max sitemaps: $MAX_SITEMAPS"
          echo "Max URLs per sitemap: $MAX_URLS_PER_SITEMAP"
          echo "Max workers: $MAX_WORKERS"
          echo "Domain: ${{ matrix.domain }}"
          
          # Check if script exists
          if [ ! -f "colemanfurniture_scraper/scripts/run.py" ]; then
            echo "❌ Script not found: colemanfurniture_scraper/scripts/run.py"
            exit 1
          fi
          
          # Run the scraper
          python colemanfurniture_scraper/scripts/run.py \
            --website-url "$CURR_URL" \
            --sitemap-offset $SITEMAP_OFFSET \
            --max-sitemaps $MAX_SITEMAPS \
            --max-urls-per-sitemap $MAX_URLS_PER_SITEMAP \
            --job-id "job${{ strategy.job-index }}" \
            --output-dir output
          
      - name: Rename output file
        run: |
          echo "Renaming output file..."
          mkdir -p output
          
          # Check if any CSV files exist
          if ls output/*.csv 1> /dev/null 2>&1; then
            CSV_FILE=$(ls output/*.csv | head -n 1)
            echo "Found CSV: $CSV_FILE"
            NEW_NAME="output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv"
            mv "$CSV_FILE" "$NEW_NAME"
            echo "Renamed to: $NEW_NAME"
          else
            echo "No CSV files found in output directory"
            # Create empty file with headers
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > "output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv"
          fi

      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.domain }}_offset_${{ matrix.offset }}
          path: output/${{ matrix.domain }}_offset_${{ matrix.offset }}.csv
          retention-days: 1

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    outputs:
      filename: ${{ steps.meta.outputs.name }}
      
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks
          
      - name: Merge CSVs
        run: |
          echo "=== Merging CSV files ==="
          
          # Find all CSV files
          CSV_FILES=$(find chunks -name "*.csv" -type f)
          
          if [ -z "$CSV_FILES" ]; then
            echo "No CSV files found to merge"
            # Create empty CSV with headers
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Status,Date Scrapped" > products_full.csv
            echo "Created empty CSV"
          else
            echo "Found $(echo "$CSV_FILES" | wc -l) CSV files"
            
            # Get first file for headers
            FIRST_FILE=$(echo "$CSV_FILES" | head -n 1)
            echo "Using headers from: $FIRST_FILE"
            
            # Extract headers (first line)
            head -n 1 "$FIRST_FILE" > products_full.csv
            
            # Merge all files (skip headers)
            TOTAL_ROWS=0
            for f in $CSV_FILES; do
              echo "Merging: $f"
              if [ -s "$f" ] && [ "$(wc -l < "$f")" -gt 1 ]; then
                FILE_ROWS=$(tail -n +2 "$f" | sed '/^$/d' | wc -l)
                TOTAL_ROWS=$((TOTAL_ROWS + FILE_ROWS))
                tail -n +2 "$f" | sed '/^$/d' >> products_full.csv
              fi
            done
            
            echo "Merged $TOTAL_ROWS rows"
          fi
          
      - name: Build output filename
        id: meta
        run: |
          # Extract the original URL from the plan step or input
          ORIGINAL_URL="${{ github.event.inputs.website_url }}"
          # Extract domain without protocol and replace dots with underscores
          CLEAN_DOMAIN=$(echo "$ORIGINAL_URL" | sed -E 's|https?://||' | sed 's|/.*||' | sed 's/\./_/g')
          DATE=$(date +%F)
          FILENAME="${CLEAN_DOMAIN}_${DATE}.csv"
          echo "name=$FILENAME" >> $GITHUB_OUTPUT
          echo "Generated filename: $FILENAME"
          
      - name: Upload final CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.name }}
          path: products_full.csv
          retention-days: 7
          
      - name: Upload to FTP server
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          echo "=== Uploading to FTP ==="
          echo "FTP Host: $FTP_HOST"
          echo "File: $FILE"
          
          # Install lftp
          sudo apt-get update && sudo apt-get install -y lftp
          
          # Upload file
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ftp:ssl-allow no
          set ssl:verify-certificate no
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put products_full.csv -o $FILE
          ls -la $FILE
          bye
          EOF
          
          echo "✅ File uploaded successfully: $FILE"
          
      - name: Cleanup
        if: always()
        run: |
          rm -rf chunks output products_full.csv
          echo "Cleanup completed"