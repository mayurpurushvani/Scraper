name: Ashley Furniture Parallel Product Scraper

on:
  workflow_dispatch:
    inputs:
      manufacturer_id:
        description: "Manufacturer ID for Ashley"
        required: true
        default: "250"
        type: string
      total_pages:
        description: "Total pages to process (0 = auto-detect)"
        default: "0"
        type: number
      pages_per_job:
        description: "Pages per parallel job (URL collection)"
        default: "20"
        type: number
      url_concurrency:
        description: "Concurrent URL requests per job"
        default: "20"
        type: number
      product_concurrency:
        description: "Concurrent product requests per chunk"
        default: "12"
        type: number
      product_chunks:
        description: "Number of parallel product scraping chunks"
        default: "8"
        type: number
      scrape_only:
        description: "Only scrape products (skip URL collection)"
        default: false
        type: boolean
      urls_file:
        description: "URLs file to use (when scrape_only is true)"
        default: ""
        type: string

jobs:
  # JOB 1: Collect URLs in parallel chunks
  collect_urls:
    if: ${{ github.event.inputs.scrape_only == 'false' }}
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      total_urls: ${{ steps.summary.outputs.total_urls }}
      urls_file: ${{ steps.summary.outputs.urls_file }}
      
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "colemanfurniture_scraper/requirements.txt" ]; then
            pip install -r colemanfurniture_scraper/requirements.txt
          else
            pip install scrapy pandas lxml requests
          fi
          
      - name: Create output directory
        run: mkdir -p output
          
      - name: Calculate parallel jobs
        id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_pages }}
          PER_JOB=${{ github.event.inputs.pages_per_job }}
          MANUFACTURER_ID="${{ github.event.inputs.manufacturer_id }}"
          
          # Ashley has ~150-200 pages
          [ "$TOTAL" -eq 0 ] && TOTAL=150
          
          # Calculate number of jobs
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))
          
          # Create matrix for parallel jobs
          MATRIX="["
          START_PAGE=1
          for ((i=0;i<JOBS;i++)); do
            END_PAGE=$(( START_PAGE + PER_JOB - 1 ))
            [ $END_PAGE -gt $TOTAL ] && END_PAGE=$TOTAL
            
            MATRIX+="{\"chunk\":$i,\"start_page\":$START_PAGE,\"end_page\":$END_PAGE,\"manufacturer_id\":\"$MANUFACTURER_ID\"},"
            START_PAGE=$(( END_PAGE + 1 ))
          done
          MATRIX="${MATRIX%,}]"
          
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Total pages: $TOTAL, Jobs: $JOBS"
          
      - name: Display job matrix
        run: |
          echo "Job matrix:"
          echo '${{ steps.matrix.outputs.matrix }}' | jq '.' || echo '${{ steps.matrix.outputs.matrix }}'

  # JOB 2: Run URL collection in parallel
  collect_chunks:
    needs: collect_urls
    if: ${{ needs.collect_urls.result == 'success' }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.collect_urls.outputs.matrix) }}
        
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "colemanfurniture_scraper/requirements.txt" ]; then
            pip install -r colemanfurniture_scraper/requirements.txt
          else
            pip install scrapy pandas lxml requests
          fi
          
      - name: Create output directory
        run: mkdir -p output
          
      - name: Collect Ashley URLs (Pages ${{ matrix.start_page }}-${{ matrix.end_page }})
        env:
          MANUFACTURER_ID: ${{ matrix.manufacturer_id }}
          START_PAGE: ${{ matrix.start_page }}
          END_PAGE: ${{ matrix.end_page }}
          CHUNK: ${{ matrix.chunk }}
          URL_CONCURRENCY: ${{ github.event.inputs.url_concurrency }}
        run: |
          echo "=== Collecting URLs: Chunk $CHUNK ==="
          echo "Pages: $START_PAGE - $END_PAGE"
          
          # Run URL collection script
          OUTPUT=$(python colemanfurniture_scraper/scripts/run_ashley_scraper.py \
            --manufacturer-id "$MANUFACTURER_ID" \
            --start-page "$START_PAGE" \
            --end-page "$END_PAGE" \
            --chunk "$CHUNK" \
            --url-concurrency "$URL_CONCURRENCY" \
            --job-id "chunk_$CHUNK" \
            --output-dir output)
          
          # Extract output file path
          OUTPUT_FILE=$(echo "$OUTPUT" | grep "OUTPUT_FILE=" | cut -d'=' -f2)
          echo "OUTPUT_FILE=$OUTPUT_FILE" >> $GITHUB_ENV
          
      - name: List output files
        run: ls -la output/
        
      - name: Upload URL chunk artifact
        uses: actions/upload-artifact@v4
        with:
          name: ashley_urls_chunk_${{ matrix.chunk }}
          path: output/ashley_urls_chunk_${{ matrix.chunk }}_*.json
          retention-days: 1

  # JOB 3: Merge URL chunks
  merge_urls:
    needs: collect_chunks
    if: ${{ needs.collect_urls.result == 'success' && needs.collect_chunks.result == 'success' }}
    runs-on: ubuntu-latest
    outputs:
      urls_file: ${{ steps.save_merged.outputs.urls_file }}
      
    steps:
      - name: Download all URL artifacts
        uses: actions/download-artifact@v4
        with:
          path: url_chunks
          
      - name: Merge URL chunks
        run: |
          echo "=== Merging URL chunks ==="
          
          # Find all JSON files
          JSON_FILES=$(find url_chunks -name "*.json" -type f)
          
          if [ -z "$JSON_FILES" ]; then
            echo "No URL files found to merge"
            exit 1
          fi
          
          echo "Found $(echo "$JSON_FILES" | wc -l) URL files"
          
          # Create merged URLs array
          echo '{"urls": [], "total_urls": 0, "manufacturer_id": "${{ github.event.inputs.manufacturer_id }}"}' > merged_urls.json
          
          TOTAL_URLS=0
          for f in $JSON_FILES; do
            echo "Processing: $f"
            # Extract URLs and append to merged file
            while read -r url; do
              if [ -n "$url" ]; then
                jq --arg url "$url" '.urls += [$url] | .total_urls += 1' merged_urls.json > tmp.json && mv tmp.json merged_urls.json
                TOTAL_URLS=$((TOTAL_URLS + 1))
              fi
            done < <(jq -r '.urls[]' "$f" 2>/dev/null || echo "")
          done
          
          # Get final count
          TOTAL_URLS=$(jq '.total_urls' merged_urls.json)
          echo "Merged $TOTAL_URLS total URLs"
          
          # Save with timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          OUTPUT_FILE="output/ashley_urls_complete_${TIMESTAMP}.json"
          mkdir -p output
          cp merged_urls.json "$OUTPUT_FILE"
          
          echo "OUTPUT_FILE=$OUTPUT_FILE" >> $GITHUB_ENV
          
      - name: Save merged URLs file info
        id: save_merged
        run: |
          echo "urls_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Upload merged URLs artifact
        uses: actions/upload-artifact@v4
        with:
          name: ashley_urls_complete
          path: output/ashley_urls_complete_*.json
          retention-days: 7

  # JOB 4: Initialize master tracking file
  init_tracking:
    needs: [merge_urls, collect_urls]
    if: ${{ always() && (github.event.inputs.scrape_only == 'true' || needs.merge_urls.result == 'success') }}
    runs-on: ubuntu-latest
    steps:
      - name: Create tracking directory
        run: mkdir -p tracking
      
      - name: Initialize master tracking file
        run: |
          DOMAIN="ashley_${{ github.event.inputs.manufacturer_id }}"
          MASTER_FILE="tracking/processed_urls_${DOMAIN}_master.json"
          
          # Create empty master file if it doesn't exist
          if [ ! -f "$MASTER_FILE" ]; then
            echo '{"urls": [], "total": 0, "last_updated": 0}' > "$MASTER_FILE"
            echo "Created new master file: $MASTER_FILE"
          else
            echo "Master file already exists: $MASTER_FILE"
            cat "$MASTER_FILE"
          fi
      
      - name: Upload tracking files
        uses: actions/upload-artifact@v4
        with:
          name: tracking-files
          path: tracking/
          retention-days: 1

  # JOB 5: Scrape products in OPTIMIZED PARALLEL CHUNKS
  scrape_products:
    needs: [merge_urls, collect_urls, init_tracking]
    if: ${{ always() && (github.event.inputs.scrape_only == 'true' || needs.merge_urls.result == 'success') }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        chunk_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
        total_chunks: [20]
        
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "colemanfurniture_scraper/requirements.txt" ]; then
            pip install -r colemanfurniture_scraper/requirements.txt
          else
            pip install scrapy pandas lxml requests
          fi
          
      - name: Create output directory
        run: mkdir -p output tracking
          
      - name: Download tracking files
        uses: actions/download-artifact@v4
        with:
          name: tracking-files
          path: tracking/
          
      - name: Determine URLs file
        id: urls_file
        run: |
          if [ "${{ github.event.inputs.scrape_only }}" = "true" ] && [ -n "${{ github.event.inputs.urls_file }}" ]; then
            # Use provided URLs file
            echo "file=${{ github.event.inputs.urls_file }}" >> $GITHUB_OUTPUT
          else
            # Download merged URLs artifact
            echo "Downloading merged URLs..."
            if [ -d "url_chunks" ]; then
              rm -rf url_chunks
            fi
            gh run download ${{ github.run_id }} -n ashley_urls_complete -D url_chunks
            COMPLETE_FILE=$(find url_chunks -name "*.json" -type f | head -n 1)
            echo "file=$COMPLETE_FILE" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}
          
      - name: Calculate optimal chunk size
        id: chunk_calc
        run: |
          URLS_FILE="${{ steps.urls_file.outputs.file }}"
          TOTAL_CHUNKS=${{ matrix.total_chunks }}
          CHUNK_ID=${{ matrix.chunk_id }}
          
          # Get total URLs
          TOTAL_URLS=$(jq '.urls | length' "$URLS_FILE")
          
          # Calculate chunk size (round up)
          CHUNK_SIZE=$(( (TOTAL_URLS + TOTAL_CHUNKS - 1) / TOTAL_CHUNKS ))
          
          START_IDX=$(( CHUNK_ID * CHUNK_SIZE ))
          END_IDX=$(( START_IDX + CHUNK_SIZE ))
          [ $END_IDX -gt $TOTAL_URLS ] && END_IDX=$TOTAL_URLS
          
          echo "total_urls=$TOTAL_URLS" >> $GITHUB_OUTPUT
          echo "chunk_size=$CHUNK_SIZE" >> $GITHUB_OUTPUT
          echo "start_idx=$START_IDX" >> $GITHUB_OUTPUT
          echo "end_idx=$END_IDX" >> $GITHUB_OUTPUT
          
      - name: Extract chunk URLs
        run: |
          URLS_FILE="${{ steps.urls_file.outputs.file }}"
          CHUNK_ID=${{ matrix.chunk_id }}
          START_IDX=${{ steps.chunk_calc.outputs.start_idx }}
          END_IDX=${{ steps.chunk_calc.outputs.end_idx }}
          
          echo "Chunk $CHUNK_ID: URLs $START_IDX to $END_IDX"
          
          # Extract chunk URLs
          jq --argjson start "$START_IDX" --argjson end "$END_IDX" \
            '{urls: .urls[$start:$end], manufacturer_id: .manufacturer_id}' \
            "$URLS_FILE" > "output/chunk_${CHUNK_ID}.json"
          
          CHUNK_COUNT=$(jq '.urls | length' "output/chunk_${CHUNK_ID}.json")
          echo "Chunk $CHUNK_ID contains $CHUNK_COUNT URLs"
          
          # Skip if no URLs
          if [ "$CHUNK_COUNT" -eq 0 ]; then
            echo "No URLs in chunk $CHUNK_ID, creating empty output"
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Images,Ref Dimensions,Ref Status,Ref Highlights,Date Scrapped" > "output/ashley_products_chunk_${CHUNK_ID}.csv"
            exit 0
          fi
          
      - name: Scrape product chunk with tracking
        if: ${{ steps.chunk_calc.outputs.chunk_size > 0 }}
        run: |
          CHUNK_ID=${{ matrix.chunk_id }}
          CHUNK_FILE="output/chunk_${CHUNK_ID}.json"
          URL_COUNT=$(jq '.urls | length' "$CHUNK_FILE")
          
          echo "=== Scraping Product Chunk $CHUNK_ID ($URL_COUNT URLs) ==="
          echo "Using concurrency: ${{ github.event.inputs.product_concurrency }}"
          
          # Copy master tracking file to current directory
          cp tracking/processed_urls_*_master.json ./ 2>/dev/null || echo "No master file found"
          
          # Run product scraper with chunk mode
          python colemanfurniture_scraper/scripts/run_ashley_scraper.py \
            --urls-file "$CHUNK_FILE" \
            --product-concurrency ${{ github.event.inputs.product_concurrency }} \
            --job-id "chunk_${CHUNK_ID}" \
            --output-dir output \
            --manufacturer-id "${{ github.event.inputs.manufacturer_id }}"
          
      - name: Rename output file
        run: |
          CHUNK_ID=${{ matrix.chunk_id }}
          
          # Find the generated CSV file
          if ls output/output_ashley_*_chunk_${CHUNK_ID}_*.csv 1> /dev/null 2>&1; then
            CSV_FILE=$(ls output/output_ashley_*_chunk_${CHUNK_ID}_*.csv | head -n 1)
            echo "Found CSV: $CSV_FILE"
            NEW_NAME="output/ashley_products_chunk_${CHUNK_ID}.csv"
            mv "$CSV_FILE" "$NEW_NAME"
            echo "Renamed to: $NEW_NAME"
          elif [ ! -f "output/ashley_products_chunk_${CHUNK_ID}.csv" ]; then
            echo "No CSV files found, creating empty file"
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Images,Ref Dimensions,Ref Status,Ref Highlights,Date Scrapped" > "output/ashley_products_chunk_${CHUNK_ID}.csv"
          fi
          
      - name: Save tracking files
        run: |
          # Save any tracking files generated by this chunk
          cp processed_urls_*.json tracking/ 2>/dev/null || echo "No tracking files to save"
          
      - name: Upload product chunk artifact
        uses: actions/upload-artifact@v4
        with:
          name: ashley_products_chunk_${{ matrix.chunk_id }}
          path: output/ashley_products_chunk_${{ matrix.chunk_id }}.csv
          retention-days: 1
          
      - name: Upload chunk tracking files
        uses: actions/upload-artifact@v4
        with:
          name: tracking-chunk-${{ matrix.chunk_id }}
          path: processed_urls_*.json
          retention-days: 1

  # JOB 6: Merge all tracking files
  merge_tracking:
    needs: scrape_products
    if: ${{ always() }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all tracking files
        uses: actions/download-artifact@v4
        with:
          path: all-tracking
          
      - name: Merge all tracking files
        run: |
          echo "=== Merging Tracking Files ==="
          
          DOMAIN="ashley_${{ github.event.inputs.manufacturer_id }}"
          MASTER_FILE="processed_urls_${DOMAIN}_master.json"
          
          # Find all tracking files
          TRACKING_FILES=$(find all-tracking -name "processed_urls_*.json" -type f ! -name "*_master.json")
          
          if [ -z "$TRACKING_FILES" ]; then
            echo "No tracking files found to merge"
            exit 0
          fi
          
          echo "Found $(echo "$TRACKING_FILES" | wc -l) tracking files"
          
          # Start with empty set
          ALL_URLS="[]"
          TOTAL=0
          
          # Merge all URLs
          for f in $TRACKING_FILES; do
            echo "Processing: $f"
            # Extract URLs and add to set
            while read -r url; do
              if [ -n "$url" ] && ! echo "$ALL_URLS" | grep -q "\"$url\""; then
                ALL_URLS=$(echo "$ALL_URLS" | jq --arg url "$url" '. + [$url]')
                TOTAL=$((TOTAL + 1))
              fi
            done < <(jq -r '.urls[]?' "$f" 2>/dev/null || echo "")
          done
          
          # Save merged master file
          echo "{\"urls\": $ALL_URLS, \"total\": $TOTAL, \"last_updated\": $(date +%s)}" > "$MASTER_FILE"
          
          echo "‚úÖ Master file created with $TOTAL unique URLs"
          
      - name: Upload master tracking file
        uses: actions/upload-artifact@v4
        with:
          name: tracking-master
          path: processed_urls_*_master.json
          retention-days: 7

  # JOB 7: Merge all product chunks, remove duplicates, and upload to FTP
  merge_and_upload:
    needs: [scrape_products, merge_urls, merge_tracking]
    if: ${{ always() && !cancelled() }}
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all product artifacts
        uses: actions/download-artifact@v4
        with:
          path: product_chunks
          merge-multiple: true
          
      - name: Download master tracking file
        uses: actions/download-artifact@v4
        with:
          name: tracking-master-${{ github.event.inputs.manufacturer_id }}
          path: tracking/
          
      - name: Merge and deduplicate product CSVs
        run: |
          echo "=== Merging and Deduplicating Product CSV Files ==="
          
          # Find all product CSV files
          CSV_FILES=$(find product_chunks -name "ashley_products_chunk_*.csv" -type f || true)
          
          if [ -z "$CSV_FILES" ]; then
            echo "No product CSV files found to merge"
            # Create empty CSV with headers
            echo "Ref Product URL,Ref Product ID,Ref Variant ID,Ref Category,Ref Category URL,Ref Brand Name,Ref Product Name,Ref SKU,Ref MPN,Ref GTIN,Ref Price,Ref Main Image,Ref Quantity,Ref Group Attr 1,Ref Group Attr 2,Ref Images,Ref Dimensions,Ref Status,Ref Highlights,Date Scrapped" > ashley_products_full.csv
            echo "Created empty CSV"
          else
            echo "Found $(echo "$CSV_FILES" | wc -l) CSV files"
            
            # Get headers from first file
            FIRST_FILE=$(echo "$CSV_FILES" | head -n 1)
            echo "Using headers from: $FIRST_FILE"
            
            # Extract headers
            HEADERS=$(head -n 1 "$FIRST_FILE")
            
            # Create a directory for processing
            mkdir -p merge_temp
            
            # Copy all CSV files to processing directory, skipping headers
            TOTAL_ROWS_BEFORE=0
            for f in $CSV_FILES; do
              if [ -s "$f" ] && [ "$(wc -l < "$f")" -gt 1 ]; then
                # Count rows in this file (excluding header)
                FILE_ROWS=$(tail -n +2 "$f" | sed '/^$/d' | wc -l)
                TOTAL_ROWS_BEFORE=$((TOTAL_ROWS_BEFORE + FILE_ROWS))
                
                # Append data (skip header)
                tail -n +2 "$f" | sed '/^$/d' >> merge_temp/all_data.csv
                echo "  + Added $FILE_ROWS rows from $(basename "$f")"
              fi
            done
            
            echo "Total rows before deduplication: $TOTAL_ROWS_BEFORE"
            
            # Check if we have any data
            if [ ! -f "merge_temp/all_data.csv" ] || [ ! -s "merge_temp/all_data.csv" ]; then
              echo "No data rows found"
              echo "$HEADERS" > ashley_products_full.csv
            else
              # Count total lines in merged file
              TOTAL_LINES=$(wc -l < merge_temp/all_data.csv)
              echo "Merged file has $TOTAL_LINES rows"
              
              # Deduplicate based on Ref Product URL (first column)
              echo "Deduplicating by Ref Product URL..."
              
              # Method 1: Using awk to keep first occurrence of each URL
              awk -F ',' '!seen[$1]++' merge_temp/all_data.csv > merge_temp/deduplicated.csv
              
              # Alternative Method 2: Using sort -u (if you prefer)
              # sort -t ',' -k1,1 -u merge_temp/all_data.csv > merge_temp/deduplicated.csv
              
              # Count after deduplication
              UNIQUE_ROWS=$(wc -l < merge_temp/deduplicated.csv)
              DUPLICATES_REMOVED=$((TOTAL_LINES - UNIQUE_ROWS))
              
              echo "‚úÖ After deduplication: $UNIQUE_ROWS unique rows (removed $DUPLICATES_REMOVED duplicates)"
              
              # Create final CSV with headers
              echo "$HEADERS" > ashley_products_full.csv
              cat merge_temp/deduplicated.csv >> ashley_products_full.csv
              
              # Show duplicate statistics by URL patterns
              echo ""
              echo "üìä Duplicate Statistics:"
              echo "  - Total rows before: $TOTAL_LINES"
              echo "  - Unique rows after: $UNIQUE_ROWS"
              echo "  - Duplicates removed: $DUPLICATES_REMOVED"
              echo "  - Duplicate rate: $(echo "scale=2; $DUPLICATES_REMOVED * 100 / $TOTAL_LINES" | bc)%"
              
              # Show sample of most duplicated URLs
              echo ""
              echo "üîç Most duplicated URLs:"
              awk -F ',' '{count[$1]++} END {for (url in count) if (count[url] > 1) print count[url], url}' merge_temp/all_data.csv | sort -nr | head -10 | while read count url; do
                echo "  - $count times: $url"
              done
            fi
            
            # Clean up temp directory
            rm -rf merge_temp
          fi
          
          # Show master tracking stats
          if [ -f "tracking/processed_urls_*_master.json" ]; then
            MASTER_FILE=$(ls tracking/processed_urls_*_master.json | head -n 1)
            TOTAL_UNIQUE=$(jq '.total' "$MASTER_FILE")
            echo ""
            echo "üìã Tracking Master File Stats:"
            echo "  - Total unique URLs tracked: $TOTAL_UNIQUE"
          fi
          
      - name: Build output filename
        id: meta
        run: |
          DATE=$(date +%Y%m%d)
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          MANUFACTURER_ID="${{ github.event.inputs.manufacturer_id }}"
          
          FILENAME="ashley_furniture_${MANUFACTURER_ID}_${DATE}.csv"
          TIMESTAMPED_FILENAME="ashley_furniture_${MANUFACTURER_ID}_${TIMESTAMP}.csv"
          
          echo "name=$FILENAME" >> $GITHUB_OUTPUT
          echo "timestamped_name=$TIMESTAMPED_FILENAME" >> $GITHUB_OUTPUT
          echo "Generated filename: $FILENAME"
          
      - name: Upload final deduplicated CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.timestamped_name }}
          path: ashley_products_full.csv
          retention-days: 7
          
      - name: Upload to FTP server
        if: success()
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
          TIMESTAMPED_FILE: ${{ steps.meta.outputs.timestamped_name }}
        run: |
          echo "=== Uploading to FTP ==="
          
          # Install lftp
          sudo apt-get update && sudo apt-get install -y lftp
          
          # Create lftp script
          cat > ftp_commands.txt << EOF
          set ftp:ssl-allow no
          set ssl:verify-certificate no
          set net:max-retries 3
          set net:timeout 30
          open -u "$FTP_USER","$FTP_PASS" "$FTP_HOST"
          mkdir -p $FTP_BASE_DIR/ashley
          cd $FTP_BASE_DIR/ashley
          
          # Upload with timestamped filename
          put ashley_products_full.csv -o $TIMESTAMPED_FILE
          echo "Uploaded: $TIMESTAMPED_FILE"
          
          # Also upload as latest (overwrite)
          put ashley_products_full.csv -o ashley_furniture_latest.csv
          echo "Uploaded: ashley_furniture_latest.csv"
          
          # Keep only last 7 daily files
          ls | grep "ashley_furniture_.*_[0-9]\{8\}\.csv" | sort | head -n -7 | xargs -r rm
          
          ls -la
          bye
          EOF
          
          # Execute lftp
          lftp -f ftp_commands.txt
          
          echo "‚úÖ Files uploaded successfully"
          
      - name: Send notification with duplicate stats
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            PRODUCT_COUNT=$(tail -n +2 ashley_products_full.csv 2>/dev/null | wc -l || echo "0")
            echo "‚úÖ Ashley scraper completed successfully!"
            echo "Total products after deduplication: $PRODUCT_COUNT"
            echo "Parallel chunks: ${{ github.event.inputs.product_chunks }}"
            
            # Show tracking stats
            if [ -f "tracking/processed_urls_*_master.json" ]; then
              MASTER_FILE=$(ls tracking/processed_urls_*_master.json | head -n 1)
              TOTAL_UNIQUE=$(jq '.total' "$MASTER_FILE")
              echo "Unique URLs tracked: $TOTAL_UNIQUE"
            fi
          else
            echo "‚ùå Ashley scraper failed with status: ${{ job.status }}"
          fi
          
      - name: Cleanup
        if: always()
        run: |
          rm -rf product_chunks url_chunks output tracking all-tracking merge_temp
          rm -f ashley_products_full.csv ftp_commands.txt processed_urls_*.json
          echo "Cleanup completed"
